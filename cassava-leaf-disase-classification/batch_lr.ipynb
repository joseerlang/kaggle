{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from src import Model, DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 256\n",
    "config = {\n",
    "    # optimization\n",
    "    'lr': 0.0009120108393559097,\n",
    "    'optimizer': 'Adam',\n",
    "    'batch_size': 64,\n",
    "    # data\n",
    "    'extra_data': 1,\n",
    "    'subset': 0.1,\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    # model\n",
    "    'backbone': 'efficientnet_b2a',\n",
    "    'pretrained': True,\n",
    "    'unfreeze': 0,\n",
    "    # data augmentation\n",
    "    'size': size,\n",
    "    'train_trans': {\n",
    "        'RandomCrop': {\n",
    "            'height': size, \n",
    "            'width': size\n",
    "        },\n",
    "        'HorizontalFlip': {},\n",
    "        'VerticalFlip': {},\n",
    "        'Normalize': {}\n",
    "    },\n",
    "    'val_trans': {\n",
    "        'CenterCrop': {\n",
    "            'height': size, \n",
    "            'width': size\n",
    "        },\n",
    "        'Normalize': {}\n",
    "    },\n",
    "    # training params\n",
    "    'precision': 16,\n",
    "    'max_epochs': 50,\n",
    "    'val_batches': 5,\n",
    "    'es_start_from': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DataModule(\n",
    "    file = 'data_extra' if config['extra_data'] else 'data_old', \n",
    "    **config\n",
    ")\n",
    "\n",
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples:  21642\n",
      "Validation samples:  5411\n",
      "Training only on 2165 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "Batch size 256 failed, trying batch size 192\n",
      "Batch size 192 failed, trying batch size 160\n",
      "Batch size 160 failed, trying batch size 144\n",
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Batch size 144 succeeded, trying batch size 152\n",
      "Batch size 152 failed, trying batch size 148\n",
      "Batch size 148 failed, trying batch size 146\n",
      "Batch size 146 failed, trying batch size 145\n",
      "Batch size 145 failed, trying batch size 144\n",
      "Finished batch size finder, will continue with full run using batch size 144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    precision=config['precision'],\n",
    "    limit_val_batches=config['val_batches'],\n",
    "    auto_scale_batch_size='binsearch'\n",
    ")\n",
    "\n",
    "trainer.tune(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"backbone\":      efficientnet_b2a\n",
       "\"batch_size\":    64\n",
       "\"es_start_from\": 0\n",
       "\"extra_data\":    1\n",
       "\"lr\":            0.0003\n",
       "\"max_epochs\":    50\n",
       "\"num_workers\":   4\n",
       "\"optimizer\":     Adam\n",
       "\"pin_memory\":    True\n",
       "\"precision\":     16\n",
       "\"pretrained\":    True\n",
       "\"size\":          256\n",
       "\"subset\":        0.1\n",
       "\"train_trans\":   {'RandomCrop': {'height': 256, 'width': 256}, 'HorizontalFlip': {}, 'VerticalFlip': {}, 'Normalize': {}}\n",
       "\"unfreeze\":      0\n",
       "\"val_batches\":   5\n",
       "\"val_trans\":     {'CenterCrop': {'height': 256, 'width': 256}, 'Normalize': {}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams.batch_size = 64\n",
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Using native 16bit precision.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    precision=config['precision'],\n",
    "    limit_val_batches=config['val_batches'],\n",
    "    auto_lr_find=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name     | Type                 | Params\n",
      "--------------------------------------------------\n",
      "0 | backbone | EfficientNetFeatures | 7.2 M \n",
      "1 | head     | Sequential           | 1.8 K \n",
      "--------------------------------------------------\n",
      "7.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66e0fd1296ce46169d430afe07d4fb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Finding best initial lr', style=ProgressStyle(descriptionâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LR finder stopped early due to diverging loss.\n"
     ]
    }
   ],
   "source": [
    "lr_finder = trainer.tuner.lr_find(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': [1e-08,\n",
       "  1.4454397707459274e-08,\n",
       "  1.7378008287493753e-08,\n",
       "  2.0892961308540398e-08,\n",
       "  2.51188643150958e-08,\n",
       "  3.019951720402016e-08,\n",
       "  3.630780547701014e-08,\n",
       "  4.36515832240166e-08,\n",
       "  5.248074602497726e-08,\n",
       "  6.309573444801934e-08,\n",
       "  7.585775750291837e-08,\n",
       "  9.120108393559096e-08,\n",
       "  1.0964781961431852e-07,\n",
       "  1.3182567385564074e-07,\n",
       "  1.5848931924611133e-07,\n",
       "  1.9054607179632475e-07,\n",
       "  2.2908676527677735e-07,\n",
       "  2.7542287033381663e-07,\n",
       "  3.311311214825911e-07,\n",
       "  3.9810717055349735e-07,\n",
       "  4.786300923226383e-07,\n",
       "  5.75439937337157e-07,\n",
       "  6.918309709189366e-07,\n",
       "  8.317637711026709e-07,\n",
       "  1e-06,\n",
       "  1.2022644346174132e-06,\n",
       "  1.445439770745928e-06,\n",
       "  1.7378008287493761e-06,\n",
       "  2.089296130854039e-06,\n",
       "  2.5118864315095797e-06,\n",
       "  3.0199517204020163e-06,\n",
       "  3.630780547701014e-06,\n",
       "  4.365158322401661e-06,\n",
       "  5.248074602497728e-06,\n",
       "  6.3095734448019305e-06,\n",
       "  7.585775750291836e-06,\n",
       "  9.120108393559096e-06,\n",
       "  1.0964781961431852e-05,\n",
       "  1.3182567385564076e-05,\n",
       "  1.584893192461114e-05,\n",
       "  1.9054607179632464e-05,\n",
       "  2.2908676527677725e-05,\n",
       "  2.7542287033381663e-05,\n",
       "  3.311311214825911e-05,\n",
       "  3.9810717055349735e-05,\n",
       "  4.786300923226385e-05,\n",
       "  5.7543993733715664e-05,\n",
       "  6.918309709189363e-05,\n",
       "  8.317637711026709e-05,\n",
       "  0.0001,\n",
       "  0.00012022644346174131,\n",
       "  0.0001445439770745928,\n",
       "  0.00017378008287493763,\n",
       "  0.0002089296130854041,\n",
       "  0.0002511886431509582,\n",
       "  0.0003019951720402019,\n",
       "  0.000363078054770101,\n",
       "  0.0004365158322401656,\n",
       "  0.0005248074602497723,\n",
       "  0.000630957344480193,\n",
       "  0.0007585775750291836,\n",
       "  0.0009120108393559097,\n",
       "  0.0010964781961431851,\n",
       "  0.0013182567385564075,\n",
       "  0.001584893192461114,\n",
       "  0.0019054607179632484,\n",
       "  0.0022908676527677745,\n",
       "  0.002754228703338169,\n",
       "  0.003311311214825908,\n",
       "  0.003981071705534969,\n",
       "  0.00478630092322638,\n",
       "  0.005754399373371567,\n",
       "  0.006918309709189364,\n",
       "  0.008317637711026709,\n",
       "  0.01,\n",
       "  0.012022644346174132,\n",
       "  0.01445439770745928,\n",
       "  0.017378008287493765,\n",
       "  0.02089296130854041,\n",
       "  0.025118864315095822,\n",
       "  0.030199517204020192,\n",
       "  0.036307805477010104,\n",
       "  0.04365158322401657,\n",
       "  0.05248074602497723,\n",
       "  0.0630957344480193,\n",
       "  0.07585775750291836,\n",
       "  0.09120108393559097,\n",
       "  0.10964781961431852,\n",
       "  0.13182567385564073,\n",
       "  0.15848931924611143,\n",
       "  0.19054607179632482,\n",
       "  0.2290867652767775,\n",
       "  0.2754228703338169,\n",
       "  0.3311311214825908,\n",
       "  0.3981071705534969,\n",
       "  0.47863009232263803],\n",
       " 'loss': [1.907150626182556,\n",
       "  1.8849517492332823,\n",
       "  1.8772222275248085,\n",
       "  1.860111980122085,\n",
       "  1.8529869267799473,\n",
       "  1.8500032290046968,\n",
       "  1.8499586477256533,\n",
       "  1.8505374697223411,\n",
       "  1.8566145818058304,\n",
       "  1.858127540092623,\n",
       "  1.8597290778572826,\n",
       "  1.8585595597144757,\n",
       "  1.8554940089343197,\n",
       "  1.8519387909198752,\n",
       "  1.849036378142202,\n",
       "  1.8426606297749173,\n",
       "  1.845422705453301,\n",
       "  1.8451876402660021,\n",
       "  1.8395926535201346,\n",
       "  1.8425678240098835,\n",
       "  1.8369539654990008,\n",
       "  1.8319391809123182,\n",
       "  1.832879795672127,\n",
       "  1.829361991568237,\n",
       "  1.829251368950088,\n",
       "  1.8270636043647401,\n",
       "  1.827857167466651,\n",
       "  1.8242643520423736,\n",
       "  1.8263301027241532,\n",
       "  1.825138044081872,\n",
       "  1.8231686746302254,\n",
       "  1.8261830614423877,\n",
       "  1.8235586742341048,\n",
       "  1.8241046267442533,\n",
       "  1.8236120310016626,\n",
       "  1.8235114180560208,\n",
       "  1.8228303404992585,\n",
       "  1.8254172147002512,\n",
       "  1.8249063654422673,\n",
       "  1.8245428741111058,\n",
       "  1.8191753474828356,\n",
       "  1.818622159569549,\n",
       "  1.8170162392610454,\n",
       "  1.816236191015855,\n",
       "  1.8119638395822406,\n",
       "  1.8101582610970128,\n",
       "  1.803642899308606,\n",
       "  1.796250671131217,\n",
       "  1.7928790470441303,\n",
       "  1.7887256150354707,\n",
       "  1.7797316392297513,\n",
       "  1.7717106690867435,\n",
       "  1.763273401062053,\n",
       "  1.7505599593641568,\n",
       "  1.7417745204886106,\n",
       "  1.72872438740534,\n",
       "  1.7141797743028573,\n",
       "  1.6998104110686136,\n",
       "  1.6877015220179192,\n",
       "  1.6738576901596562,\n",
       "  1.6584199744909147,\n",
       "  1.63667591695703,\n",
       "  1.6170643708223527,\n",
       "  1.5993480714279595,\n",
       "  1.5782123130266394,\n",
       "  1.5623930359066358,\n",
       "  1.5442708252863735,\n",
       "  1.5298201416236223,\n",
       "  1.512291601319965,\n",
       "  1.4958383308918721,\n",
       "  1.4747901712249327,\n",
       "  1.4623534939936846,\n",
       "  1.4490553814205362,\n",
       "  1.4449425248534102,\n",
       "  1.4350996941134637,\n",
       "  1.4315134542586199,\n",
       "  1.4248016215907533,\n",
       "  1.4215975919879011,\n",
       "  1.4106926879049744,\n",
       "  1.4039275902387827,\n",
       "  1.402394233705212,\n",
       "  1.3932406139185067,\n",
       "  1.3883898321301948,\n",
       "  1.3941079246238555,\n",
       "  1.3968055275360154,\n",
       "  1.4245396360746818,\n",
       "  1.451776213407234,\n",
       "  1.4836663706055622,\n",
       "  1.4970789186432438,\n",
       "  1.5643113469740173,\n",
       "  1.634785859933816,\n",
       "  1.7175146127392549,\n",
       "  2.15974666457261,\n",
       "  2.5031963680768015,\n",
       "  2.7599194233904423,\n",
       "  6.133630678407642]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_finder.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009120108393559097"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch)",
   "language": "python",
   "name": "conda_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
